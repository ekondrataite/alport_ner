##################### Libraries ###################################################

import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
import spacy
from collections import Counter
import nltk
from nltk import word_tokenize
from nltk.util import ngrams

##################### Primary Analysis ###################################################

##################### Record Counts

unique_patient_counts = alport_df.groupby('source')['subject_id'].nunique()
unique_recod_counts = alport_df.groupby('source')['note_id'].nunique()
unique_category_counts = alport_df.groupby('source')['category'].nunique()

print("Unique patient counts:")
print(unique_patient_counts)

print("\nUnique record counts:")
print(unique_recod_counts)

print("\nUnique category counts:")
print(unique_category_counts)

# Counts for each category

unique_categories = alport_df['category'].value_counts()
unique_categories

patient_records = alport_df['subject_id'].value_counts()

print("Max: {} \nMin: {} \nAverage: {}".format(max(patient_records), min(patient_records), patient_records.mean()))

# Plotting bar chart
plt.figure(figsize=(8, 5))
patient_records.plot(kind='bar', alpha=0.8, label='_nolegend_')
plt.xlabel("\nPatients", size=12)
plt.ylabel("Record Count\n", size=12)
plt.xticks([])
plt.yticks(np.arange(0, max(patient_records) + 10, 10))
plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)
plt.axhline(y=patient_records.mean(), color='red', linestyle='--', linewidth=1, label=f'Average Record Count')
plt.legend(fontsize=12)

# Customize axes
ax = plt.gca()
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_color('gray')
ax.spines['bottom'].set_color('gray')

##################### Word Counts

# Simple pre-processing for word counts

alport_df["for_counts"] = alport_df["text"].str.lower()
alport_df["for_counts"] = alport_df["for_counts"].str.replace(r'[\d' + string.punctuation + ']', '', regex=True)
alport_df["for_counts"] = alport_df["for_counts"].str.replace(r'\s+', ' ', regex=True).str.strip()

alport_df['word_count'] = alport_df['for_counts'].str.split().str.len()

# Calculate minimal, maximum, and average word count
min_word_count = alport_df['word_count'].min()
max_word_count = alport_df['word_count'].max()
avg_word_count = alport_df['word_count'].mean()

words = alport_df['for_counts'].str.cat(sep=' ').split()

number_of_words = len(words)
num_unique_words = len(set(words))

# Display the results
print("\nOverall Word Count Statistics:")
print(f"Minimum Word Count: {min_word_count}")
print(f"Maximum Word Count: {max_word_count}")
print(f"Average Word Count: {avg_word_count:.2f}")

print("Number of words across all records:", number_of_words)
print("Number of unique words across all records:", num_unique_words)

# Statistics by category
category_stats = alport_df.groupby('category').agg(
    min_word_count=('word_count', 'min'),
    max_word_count=('word_count', 'max'),
    avg_word_count=('word_count', 'mean'),
    total_words=('for_counts', lambda x: len(' '.join(x).split())),
    num_unique_words=('for_counts', lambda x: len(set(' '.join(x).split())))
).reset_index()

print("\nWord Count Statistics by Category:")
print(category_stats.sort_values(by='total_words', ascending=False))

##################### Analysis for common words

# First remove stop words so that they do not interfere

nlp_classic = spacy.load("en_core_web_sm")

def remove_stopwords(text):
    doc = nlp_classic(text.lower())
    tokens = [token.text for token in doc if not token.is_stop]
    clean_text = ' '.join(tokens)
    return clean_text.strip()

alport_df["for_common_words"] = alport_df["for_counts"].apply(remove_stopwords)

# Removing extra document specific stop words

doc_specific_words_to_remove = [
    'admission date', 'discharge date', '\n', 'date birth', 'discharge instructions', 'admission',
    'mg', 'tablet', 'po', 'patient', 'hospital', 'pt', 'right', 'left', 'daily', 'discharge',
    'pm', 'dr', ' x ', 'noted', 'history', 'day', 'namepattern', 'sp', 'sig', 'w', 'doctor',
    'lastname', 'spouse', 'normal', 'chest', 'findings', 'evidence', 'ct', 'seen', 'year', 'old',
    'man', 'female'
]

pattern = r'\b(?:' + '|'.join(doc_specific_words_to_remove) + r')\b'

alport_df['for_common_words'] = alport_df['for_common_words'].str.replace(pattern, '', regex=True).str.replace(r'\s+', ' ', regex=True).str.strip()

def find_most_common_words(df, column_name, num_words=10):
    text_data = ' '.join(df[column_name].dropna().astype(str))
    words = text_data.split()
    word_counts = Counter(words)
    most_common_words = word_counts.most_common(num_words)

    word_occurrences = pd.DataFrame(most_common_words, columns=['Word', 'Count'])

    word_occurrences['Record_count'] = word_occurrences['Word'].apply(lambda term: df[column_name].str.contains(term).sum())

    return word_occurrences


def find_most_common_bigrams(df, column_name, subject_column, n=10):
    all_text = ' '.join(df[column_name].dropna().tolist())
    tokens = word_tokenize(all_text.lower())
    bigrams = list(ngrams(tokens, 3))

    bigram_counts = Counter(bigrams)

    most_common_bigrams = bigram_counts.most_common(n)
    bigram_occurrences = pd.DataFrame(most_common_bigrams, columns=['Bigram', 'Count'])

# Example how the analysis was done

most_common_df = find_most_common_words(alport_df, 'for_common_words', num_words=60)
most_common_df.tail(10)

most_common_bigrams = find_most_common_bigrams(alport_df, 'for_common_words', 'subject_id', n=80)
most_common_bigrams.tail(10)

    bigram_occurrences['Record_count'] = bigram_occurrences['Bigram'].apply(lambda bigram: df[column_name].str.contains(' '.join(bigram)).sum())

    def count_patients_for_bigram(bigram):
        matching_records = df[df[column_name].str.contains(' '.join(bigram))]
        unique_patients = matching_records[subject_column].nunique()
        return unique_patients

    bigram_occurrences['Patient_count'] = bigram_occurrences['Bigram'].apply(count_patients_for_bigram)

    return bigram_occurrences.sort_values(by='Patient_count', ascending=False)
